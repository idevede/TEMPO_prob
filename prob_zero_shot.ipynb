{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual, vali, test\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "from models.PatchTST import PatchTST\n",
    "from models.GPT4TS import GPT4TS\n",
    "from models.DLinear import DLinear\n",
    "from models.TEMPO import TEMPO\n",
    "# from models.T5 import T54TS\n",
    "from models.ETSformer import ETSformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from numpy.random import choice\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from numpy.random import choice\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_config(config_path=None):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to ./configs/custom_dataset.yml\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Define the configuration dictionary\n",
    "config = {\n",
    "    \"datasets\": {\n",
    "        \"Taxi\": {\n",
    "            \"root_path\": \"/u/dcao1/workspace/Generative-TS-train_all/csdi_data/taxi_30min/\",\n",
    "            \"data_path\": \"train\",\n",
    "            \"data_name\": \"Taxi\",\n",
    "            \"data\": \"taxi\",\n",
    "            \"lradj\": \"type4\",\n",
    "            \"features\": \"M\",\n",
    "            \"target\": \"400001\",\n",
    "            \"embed\": \"timeF\",\n",
    "            \"freq\": 0,\n",
    "            \"percent\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an OmegaConf object from the dictionary\n",
    "cfg = OmegaConf.create(config)\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(\"./configs/custom_dataset.yml\", \"w\") as f:\n",
    "    OmegaConf.save(cfg, f)\n",
    "\n",
    "print(\"Configuration saved to ./configs/custom_dataset.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration written to config.yml\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"description\": \"TEMPO\",\n",
    "    \"checkpoints\": \"checkpoints/lora_revin_5domain_checkpoints_1/\",\n",
    "    \"task_name\": \"long_term_forecast\",\n",
    "    \"prompt\": 1,\n",
    "    \"num_nodes\": 1,\n",
    "    \"seq_len\": 168,\n",
    "    \"pred_len\": 30,\n",
    "    \"label_len\": 30,\n",
    "    \"decay_fac\": 0.5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 0,\n",
    "    \"train_epochs\": 10,\n",
    "    \"lradj\": \"type3\",\n",
    "    \"patience\": 5,\n",
    "    \"gpt_layers\": 6,\n",
    "    \"is_gpt\": 1,\n",
    "    \"e_layers\": 3,\n",
    "    \"d_model\": 768,\n",
    "    \"n_heads\": 4,\n",
    "    \"d_ff\": 768,\n",
    "    \"dropout\": 0.3,\n",
    "    \"enc_in\": 7,\n",
    "    \"c_out\": 1,\n",
    "    \"patch_size\": 16,\n",
    "    \"kernel_size\": 25,\n",
    "    \"loss_func\": \"mse\",\n",
    "    \"pretrain\": 1,\n",
    "    \"freeze\": 1,\n",
    "    \"model\": \"TEMPO\",\n",
    "    \"stride\": 8,\n",
    "    \"max_len\": -1,\n",
    "    \"hid_dim\": 16,\n",
    "    \"tmax\": 20,\n",
    "    \"itr\": 3,\n",
    "    \"cos\": 1,\n",
    "    \"equal\": 1,\n",
    "    \"pool\": False,\n",
    "    \"no_stl_loss\": False,\n",
    "    \"stl_weight\": 0.001,\n",
    "    \"config_path\": \"./configs/custom_dataset.yml\",\n",
    "    \"datasets\": \"ETTm1,ETTh1,ETTm2,electricity,traffic,weather\",\n",
    "    \"target_data\": \"Taxi\",\n",
    "    \"use_token\": 0,\n",
    "    \"electri_multiplier\": 1,\n",
    "    \"traffic_multiplier\": 1,\n",
    "    \"embed\": \"timeF\",\n",
    "    \"percent\": 100,\n",
    "    # \"model_id\": 'prob_TEMPO_6_prompt_learn_168_24_100_sl336_ll168_pl24_dm768_nh4_el3_gl6_df768_ebtimeF_itr0' #\"prob_TEMPO_6_prompt_learn_168_24_100_sl336_ll30_pl30_dm768_nh4_el3_gl6_df768_ebtimeF_itr0\"\n",
    "    \"model_id\": \"prob_TEMPO_6_prompt_learn_168_30_100_sl336_ll30_pl30_dm768_nh4_el3_gl6_df768_ebtimeF_itr0\"\n",
    "}\n",
    "\n",
    "# Create an OmegaConf object from the dictionary\n",
    "cfg = OmegaConf.create(config)\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(\"configs/etth2_config.yml\", \"w\") as f:\n",
    "    OmegaConf.save(cfg, f)\n",
    "\n",
    "print(\"Configuration written to config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_init_config(cfg.config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONALITY_MAP = {\n",
    "   \"minutely\": 1440,\n",
    "   \"10_minutes\": 144,\n",
    "   \"half_hourly\": 48,\n",
    "   \"hourly\": 24,\n",
    "   \"daily\": 7,\n",
    "   \"weekly\": 1,\n",
    "   \"monthly\": 12,\n",
    "   \"quarterly\": 4,\n",
    "   \"yearly\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 308736 || all params: 82207488 || trainable%: 0.38\n"
     ]
    }
   ],
   "source": [
    "if cfg.model == 'PatchTST':\n",
    "    model = PatchTST(cfg, device)\n",
    "    model.to(device)\n",
    "elif cfg.model == 'DLinear':\n",
    "    model = DLinear(cfg, device)\n",
    "    model.to(device)\n",
    "elif cfg.model == 'TEMPO':\n",
    "    model = TEMPO(cfg, device)\n",
    "    model.to(device)\n",
    "elif cfg.model == 'T5':\n",
    "    model = T54TS(cfg, device)\n",
    "    model.to(device)\n",
    "elif 'ETSformer' in cfg.model:\n",
    "    model = ETSformer(cfg, device)\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = GPT4TS(cfg, device)\n",
    "\n",
    "\n",
    "params = model.parameters()\n",
    "model_optim = torch.optim.Adam(params, lr=cfg.learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=cfg.patience, verbose=True)\n",
    "if cfg.loss_func == 'mse':\n",
    "    criterion = nn.MSELoss()\n",
    "elif cfg.loss_func == 'smape':\n",
    "    class SMAPE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SMAPE, self).__init__()\n",
    "        def forward(self, pred, true):\n",
    "            return torch.mean(200 * torch.abs(pred - true) / (torch.abs(pred) + torch.abs(true) + 1e-8))\n",
    "    criterion = SMAPE()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=cfg.tmax, eta_min=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/lora_revin_5domain_checkpoints_1/prob_TEMPO_6_prompt_learn_168_30_100_sl336_ll30_pl30_dm768_nh4_el3_gl6_df768_ebtimeF_itr0/checkpoint.pth\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(cfg.checkpoints, cfg.model_id)\n",
    "best_model_path = model_path + '/checkpoint.pth'\n",
    "print(best_model_path)\n",
    "model.load_state_dict(torch.load(best_model_path,  map_location=torch.device(device)), strict=False)\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Taxi': {'root_path': '/u/dcao1/workspace/Generative-TS-train_all/csdi_data/taxi_30min/', 'data_path': 'train', 'data_name': 'Taxi', 'data': 'taxi', 'lradj': 'type4', 'features': 'M', 'target': '400001', 'embed': 'timeF', 'freq': 0, 'percent': 100}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['datasets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.enc_in = 1214\n",
      "self.data_x = (57, 1214)\n",
      "test 57\n"
     ]
    }
   ],
   "source": [
    "cfg.data = config['datasets'][cfg.target_data].data\n",
    "cfg.root_path = config['datasets'][cfg.target_data].root_path\n",
    "cfg.data_path = config['datasets'][cfg.target_data].data_path\n",
    "cfg.data_name = config['datasets'][cfg.target_data].data_name\n",
    "cfg.features = config['datasets'][cfg.target_data].features\n",
    "cfg.freq = config['datasets'][cfg.target_data].freq\n",
    "cfg.target = config['datasets'][cfg.target_data].target\n",
    "cfg.embed = config['datasets'][cfg.target_data].embed\n",
    "cfg.percent = config['datasets'][cfg.target_data].percent\n",
    "cfg.lradj = config['datasets'][cfg.target_data].lradj\n",
    "if cfg.freq == 0:\n",
    "    cfg.freq = 'h'\n",
    "cfg.pred_len = 24 \n",
    "cfg.seq_len = 48+cfg.pred_len #168 + 24\n",
    "\n",
    "test_data, test_loader = data_provider(cfg, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n",
      "torch.Size([35, 1214, 30])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch.distributions as dist\n",
    "preds = []\n",
    "trues = []\n",
    "history = []\n",
    "# mases = []\n",
    "\n",
    "# Initialize accumulators for errors\n",
    "total_mae = 0\n",
    "total_mse = 0\n",
    "n_samples = 0\n",
    "itr = 0\n",
    "\n",
    "cfg.seq_len = cfg.seq_len - cfg.pred_len\n",
    "# cfg.pred_len = 24\n",
    "model.eval()\n",
    "# import pdb; pdb.set_trace()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_data)):\n",
    "        #enumerate(test_loader): #tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        data = test_data[i]\n",
    "        seq_x, text_embedding, observed_mask = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        seq_x = seq_x.float().to(device)\n",
    "        # print(seq_x.shape)\n",
    "        batch_x = seq_x[:, :cfg.seq_len].unsqueeze(-1) # 137, 168, 1\n",
    "        history.append(batch_x.cpu().numpy())\n",
    "        if batch_x.shape[1] < 168:\n",
    "            # zeros_padding = torch.zeros((batch_x.shape[0], 168-batch_x.shape[1], batch_x.shape[2])).to(device)\n",
    "            # batch_x = torch.cat([zeros_padding, batch_x], dim=1)\n",
    "            # print(168 - batch_x.shape[1], batch_x.shape[1])\n",
    "            # n = 168 - batch_x.shape[1]\n",
    "            # last_elements = batch_x[:, -n:, :]\n",
    "            # # print(last_elements.shape)\n",
    "            # batch_x = torch.cat([last_elements, batch_x], dim=1)\n",
    "            # # print(batch_x.shape)\n",
    "            zeros_padding = torch.zeros((batch_x.shape[0], 168-batch_x.shape[1], batch_x.shape[2])).to(device)\n",
    "            batch_x = torch.cat([zeros_padding, batch_x], dim=1)\n",
    "        batch_y = seq_x[:,cfg.seq_len:cfg.seq_len+cfg.pred_len]\n",
    "        trues.append(batch_y.cpu().numpy())\n",
    "        \n",
    "                        \n",
    "        outputs, _ = model(batch_x, 0, None, None, None) #+ model(seq_seasonal, ii) + model(seq_resid, ii)\n",
    "\n",
    "        mu, sigma, nu = outputs[0], outputs[1], outputs[2]\n",
    "        # Create the Student's t-distribution with the predicted parameters\n",
    "        student_t = dist.StudentT(df=nu, loc=mu, scale=sigma)\n",
    "\n",
    "        # Generate 30 samples for each prediction\n",
    "        num_samples = 35\n",
    "        probabilistic_forecasts = student_t.rsample((num_samples,))\n",
    "\n",
    "        # The shape of probabilistic_forecasts will be (num_samples, batch_size, pred_length)\n",
    "        print(probabilistic_forecasts.shape)\n",
    "        preds.append(probabilistic_forecasts.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = np.array(trues)\n",
    "synthetic_data = np.array(preds)\n",
    "history_data = np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = synthetic_data[:,:,:, :gt_data.shape[2]]\n",
    "target_mask = np.ones_like(gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_data = np.swapaxes(synthetic_data, -1, -2)\n",
    "# gt_data = np.swapaxes(gt_data, -1, -2)\n",
    "# synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.imputation_metrics import mse_withmask, mae_withmask, calc_quantile_CRPS, calc_quantile_CRPS_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662455709357011"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_quantile_CRPS_sum(torch.Tensor(gt_data),torch.Tensor(synthetic_data),torch.Tensor(target_mask),mean_scaler=0,scaler=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unormzalized_his_data = []\n",
    "for g in history_data:\n",
    "    unormzalized_his_data.append(np.transpose(np.transpose(g)*test_data.std_data+test_data.mean_data))\n",
    "unormzalized_his_data = np.array(unormzalized_his_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 1214, 24), (57, 35, 1214, 24))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unormzalized_gt_data = []\n",
    "for g in gt_data:\n",
    "    unormzalized_gt_data.append(np.transpose(np.transpose(g)*test_data.std_data+test_data.mean_data))\n",
    "unormzalized_gt_data = np.array(unormzalized_gt_data)\n",
    "unormalized_synthetic_data = []\n",
    "for i in range(len(synthetic_data)):\n",
    "    for j in range(len(synthetic_data[i])):\n",
    "        s = synthetic_data[i][j]\n",
    "        unormalized_synthetic_data.append(np.transpose(np.transpose(s)*test_data.std_data+test_data.mean_data))\n",
    "unormalized_synthetic_data = np.array(unormalized_synthetic_data).reshape(synthetic_data.shape[0],-1,synthetic_data.shape[2],synthetic_data.shape[3])\n",
    "unormzalized_gt_data.shape,unormalized_synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unormalized_synthetic_data = np.swapaxes(unormalized_synthetic_data, -1, -2)\n",
    "unormzalized_gt_data = np.swapaxes(unormzalized_gt_data, -1, -2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = np.ones_like(unormzalized_gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3519637459202817"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_quantile_CRPS_sum(torch.Tensor(unormzalized_gt_data),torch.Tensor(unormalized_synthetic_data),torch.Tensor(target_mask),mean_scaler=0,scaler=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:   CRPS_sum:\n",
      "0.3519637459202817\n",
      "all: CRPS_sum:\n",
      "Mean CRPS_sum: 0.35284811990302906\n",
      "Standard Deviation CRPS_sum: 0.0001372056193224073\n"
     ]
    }
   ],
   "source": [
    "print(\"all:   CRPS_sum:\")\n",
    "print(calc_quantile_CRPS_sum(torch.Tensor(unormzalized_gt_data),torch.Tensor(unormalized_synthetic_data),torch.Tensor(target_mask),mean_scaler=0,scaler=1))\n",
    "print(\"all: CRPS_sum:\")\n",
    "# import pdb; pdb.set_trace()\n",
    "CRPS_sum_1 = calc_quantile_CRPS_sum(torch.Tensor(unormzalized_gt_data),torch.Tensor(unormalized_synthetic_data[:,:10]),torch.Tensor(target_mask),mean_scaler=0,scaler=1)\n",
    "CRPS_sum_2 = calc_quantile_CRPS_sum(torch.Tensor(unormzalized_gt_data),torch.Tensor(unormalized_synthetic_data[:,10:20]),torch.Tensor(target_mask),mean_scaler=0,scaler=1)\n",
    "CRPS_sum_3 = calc_quantile_CRPS_sum(torch.Tensor(unormzalized_gt_data),torch.Tensor(unormalized_synthetic_data[:,20:30]),torch.Tensor(target_mask),mean_scaler=0,scaler=1)\n",
    "\n",
    "CRPS_sum_mean = np.mean([CRPS_sum_1, CRPS_sum_2, CRPS_sum_3])\n",
    "CRPS_sum_std = np.std([CRPS_sum_1, CRPS_sum_2, CRPS_sum_3])\n",
    "print(\"Mean CRPS_sum:\", CRPS_sum_mean)\n",
    "print(\"Standard Deviation CRPS_sum:\", CRPS_sum_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_q = np.quantile(synthetic_data,0.05,axis=1)\n",
    "high_q = np.quantile(synthetic_data,0.95,axis=1)\n",
    "mid_q = np.quantile(synthetic_data,0.5,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 1214, 48, 1), (57, 1214, 24))"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_data.shape, gt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 1214, 72)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_data = np.concatenate([history_data.squeeze(), gt_data], axis=2)\n",
    "traj_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.pred_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (57, 1214, 72)\n",
      "1 (57, 1214, 72)\n",
      "2 (57, 1214, 72)\n",
      "3 (57, 1214, 72)\n",
      "4 (57, 1214, 72)\n",
      "5 (57, 1214, 72)\n",
      "6 (57, 1214, 72)\n",
      "7 (57, 1214, 72)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "i = 0\n",
    "#3000\n",
    "seq_len = 72 #120 #92\n",
    "feat_dim = synthetic_data.shape[2]\n",
    "feat_dim = 8\n",
    "fig, axes = plt.subplots(nrows=feat_dim, ncols=1, figsize=(10, 20))\n",
    "for feat_idx in range(feat_dim):\n",
    "    # df_x = pd.DataFrame({\"x\": np.arange(0, seq_len), \"val\": gt_data[i, feat_idx, :],\n",
    "    #                      \"y\": (1-target_mask)[i, feat_idx, :]})\n",
    "    # df_x = df_x[df_x.y!=0]\n",
    "    print(feat_idx, traj_data.shape)\n",
    "    df_o = pd.DataFrame({\"x\": np.arange(0, seq_len), \"val\": traj_data[i, feat_idx, :]})\n",
    "                        # \"y\": target_mask[i, feat_idx, :]})\n",
    "    # df_o = df_o[df_o.y!=0]\n",
    "    axes[feat_idx].plot(df_o.x, df_o.val, color='b',  linestyle='solid', label='label')\n",
    "    # axes[feat_idx].plot(df_x.x, df_x.val, color='r', marker='x', linestyle='None')\n",
    "    axes[feat_idx].plot(range(seq_len-cfg.pred_len, seq_len), mid_q[i, feat_idx, -cfg.pred_len:], color='g', linestyle='solid', label='Diffusion-TS')\n",
    "    axes[feat_idx].fill_between(range(seq_len-cfg.pred_len, seq_len), low_q[i, feat_idx,-cfg.pred_len:],high_q[i,feat_idx, -cfg.pred_len:], color='g', alpha = 0.3)\n",
    "    plt.setp(axes[feat_idx], ylabel='value')\n",
    "    if feat_idx == feat_dim-1:\n",
    "        plt.setp(axes[-1], xlabel='time')\n",
    "    # plt.show()\n",
    "    # axes[feat_idx].set_ylim(-3, 3)\n",
    "plt.legend()\n",
    "plt.savefig('traffic_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.WIS import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = np.swapaxes(unormalized_synthetic_data, -1, -2)\n",
    "\n",
    "labels = np.swapaxes(unormzalized_gt_data, -1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_q = np.quantile(synthetic_data,0.05,axis=1)\n",
    "high_q = np.quantile(synthetic_data,0.95,axis=1)\n",
    "mid_q = np.quantile(synthetic_data,0.5,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69198, 24) (69198, 24) (69198, 24)\n"
     ]
    }
   ],
   "source": [
    "high_q_reshape = np.reshape(high_q,newshape=(-1,high_q.shape[-1]))\n",
    "low_q_reshape = np.reshape(low_q,newshape=(-1,low_q.shape[-1]))\n",
    "gt_data_reshape = np.reshape(labels,newshape=(-1,labels.shape[-1]))\n",
    "print(high_q_reshape.shape,low_q_reshape.shape,gt_data_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # to be set by competition hosts\n",
    "if not(alpha > 0 and alpha <= 1):\n",
    "    raise ParticipantVisibleError(f'alpha should be (0,1]. Found {alpha}')\n",
    "\n",
    "MWIS,coverage = score(gt_data_reshape,high_q_reshape,low_q_reshape,alpha)\n",
    "\n",
    "if not(coverage >= (1-alpha)):\n",
    "    raise ParticipantVisibleError(f'Coverage was below {(1-alpha)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91.67641494790806, 10.50403190843666)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWIS,coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
